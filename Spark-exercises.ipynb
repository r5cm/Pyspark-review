{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook solves PySpark exercises, using both RDDs and Spark DataFrames. Original exercises were taken from [Six Spark Exercises to Rule Them All](https://towardsdatascience.com/six-spark-exercises-to-rule-them-all-242445b24565)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema of the dataset can be seen in image below.\n",
    "\n",
    "![dataset](https://miro.medium.com/max/700/1*wA4xJu3LMcm_vR5pFJkLpA.png)\n",
    "\n",
    "The metadata of the table is the following:\n",
    "\n",
    "- Sales\n",
    "    - `order_id`: The order ID\n",
    "    - `product_id`: The single product sold in the order. All orders have exactly one product)\n",
    "    - `seller_id`: The selling employee ID that sold the product\n",
    "    - `num_pieces_sold`: The number of units sold for the specific product in the order\n",
    "    - `bill_raw_text`: A string that represents the raw text of the bill associated with the order\n",
    "    - `date`: The date of the order.\n",
    "- Products\n",
    "    - `product_id`: The product ID\n",
    "    - `product_name`: The product name\n",
    "    - `price`: The product price\n",
    "- Sellers\n",
    "    - `seller_id`: The seller ID\n",
    "    - `seller_name`: The seller name\n",
    "    - `daily_target`: The number of items (regardless of the product type) that the seller needs to hit his/her quota. For example, if the daily target is 100,000, the employee needs to sell 100,000 products he can hit the quota by selling 100,000 units of product_0, but also selling 30,000 units of product_1 and 70,000 units of product_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is generated with the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products between 1 and 75000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74999999/74999999 [09:43<00:00, 128453.74it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 28706.26it/s]\n",
      "100%|██████████| 40/40 [1:29:58<00:00, 134.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "import string\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "random.seed(1999)\n",
    "\n",
    "letters = string.ascii_lowercase\n",
    "letters_upper = string.ascii_uppercase\n",
    "for _i in range(0, 10):\n",
    "    letters += letters\n",
    "\n",
    "for _i in range(0, 10):\n",
    "    letters += letters_upper\n",
    "\n",
    "\n",
    "def random_string(stringLength=10):\n",
    "    \"\"\"Generate a random string of fixed length \"\"\"\n",
    "    return ''.join(random.sample(letters, stringLength))\n",
    "\n",
    "\n",
    "print(\"Products between {} and {}\".format(1, 75000000))\n",
    "product_ids = [x for x in range(1, 75000000)]\n",
    "dates = ['2020-07-01', '2020-07-02', '2020-07-03', '2020-07-04', '2020-07-05', '2020-07-06', '2020-07-07', '2020-07-08',\n",
    "         '2020-07-09', '2020-07-10']\n",
    "seller_ids = [x for x in range(1, 10)]\n",
    "\n",
    "\n",
    "#   Generate products\n",
    "products = [[0, \"product_0\", 22]]\n",
    "for p in tqdm(product_ids):\n",
    "    products.append([p, \"product_{}\".format(p), random.randint(1, 150)])\n",
    "#   Save dataframe\n",
    "df = pd.DataFrame(products)\n",
    "df.columns = [\"product_id\", \"product_name\", \"price\"]\n",
    "df.to_csv(\"data/products.csv\", index=False)\n",
    "del df\n",
    "del products\n",
    "\n",
    "#   Generate sellers\n",
    "sellers = [[0, \"seller_0\", 2500000]]\n",
    "for s in tqdm(seller_ids):\n",
    "    sellers.append([s, \"seller_{}\".format(s), random.randint(12000, 2000000)])\n",
    "#   Save dataframe\n",
    "df = pd.DataFrame(sellers)\n",
    "df.columns = [\"seller_id\", \"seller_name\", \"daily_target\"]\n",
    "df.to_csv(\"data/sellers.csv\", index=False)\n",
    "\n",
    "#   Generate sales\n",
    "total_rows = 500000\n",
    "prod_zero = int(total_rows * 0.95)\n",
    "prod_others = total_rows - prod_zero + 1\n",
    "df_array = [[\"order_id\", \"product_id\", \"seller_id\", \"date\", \"num_pieces_sold\", \"bill_raw_text\"]]\n",
    "with open('data/sales.csv', 'w', newline='') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerows(df_array)\n",
    "\n",
    "order_id = 0\n",
    "for i in tqdm(range(0, 40)):\n",
    "    df_array = []\n",
    "\n",
    "    for i in range(0, prod_zero):\n",
    "        order_id += 1\n",
    "        df_array.append([order_id, 0, 0, random.choice(dates), random.randint(1, 100), random_string(500)])\n",
    "\n",
    "    with open('data/sales.csv', 'a', newline='') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "        csvwriter.writerows(df_array)\n",
    "\n",
    "    df_array = []\n",
    "    for i in range(0, prod_others):\n",
    "        order_id += 1\n",
    "        df_array.append(\n",
    "            [order_id, random.choice(product_ids), random.choice(seller_ids), random.choice(dates),\n",
    "             random.randint(1, 100), random_string(500)])\n",
    "\n",
    "    with open('data/sales.csv', 'a', newline='') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "        csvwriter.writerows(df_array)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And stored to Parquet using the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/21 14:11:11 WARN Utils: Your hostname, MSI resolves to a loopback address: 127.0.1.1; using 172.17.179.177 instead (on interface eth0)\n",
      "22/06/21 14:11:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/21 14:11:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "+----------+------------+-----+\n",
      "|product_id|product_name|price|\n",
      "+----------+------------+-----+\n",
      "|         0|   product_0|   22|\n",
      "|         1|   product_1|   30|\n",
      "|         2|   product_2|   91|\n",
      "|         3|   product_3|   37|\n",
      "|         4|   product_4|  145|\n",
      "|         5|   product_5|  128|\n",
      "|         6|   product_6|   66|\n",
      "|         7|   product_7|  145|\n",
      "|         8|   product_8|   51|\n",
      "|         9|   product_9|   44|\n",
      "|        10|  product_10|   53|\n",
      "|        11|  product_11|   13|\n",
      "|        12|  product_12|  104|\n",
      "|        13|  product_13|  102|\n",
      "|        14|  product_14|   24|\n",
      "|        15|  product_15|   14|\n",
      "|        16|  product_16|   38|\n",
      "|        17|  product_17|   72|\n",
      "|        18|  product_18|   16|\n",
      "|        19|  product_19|   46|\n",
      "+----------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|         0|        0|2020-07-03|             98|frlnwjcoaxsaubnat...|\n",
      "|       2|         0|        0|2020-07-07|             23|zsnrbwrlflvqqmbcz...|\n",
      "|       3|         0|        0|2020-07-02|             79|gmxnirkafafnohboh...|\n",
      "|       4|         0|        0|2020-07-07|              5|xrgknaskXkfcxcnzj...|\n",
      "|       5|         0|        0|2020-07-10|             79|tzkqoynsqnfomkpbt...|\n",
      "|       6|         0|        0|2020-07-05|             87|qoluiczrckaygkzbi...|\n",
      "|       7|         0|        0|2020-07-08|             14|ivwpwrpuhrjgjdauj...|\n",
      "|       8|         0|        0|2020-07-02|             64|hoalxshwHpqgyvqtm...|\n",
      "|       9|         0|        0|2020-07-02|             45|vysrvsdfvekabcmwo...|\n",
      "|      10|         0|        0|2020-07-05|             16|poiemeiqharpjqkao...|\n",
      "|      11|         0|        0|2020-07-09|              4|badjqluozzjHbbjkv...|\n",
      "|      12|         0|        0|2020-07-02|             58|fdgikecrmegaxfpvO...|\n",
      "|      13|         0|        0|2020-07-02|             56|zhrkbicjlasuqqwsl...|\n",
      "|      14|         0|        0|2020-07-04|             43|sivmclqcgiaspgomj...|\n",
      "|      15|         0|        0|2020-07-05|             39|usobmyZrxjdzdrecl...|\n",
      "|      16|         0|        0|2020-07-04|             79|zxbixfkhmydtewfje...|\n",
      "|      17|         0|        0|2020-07-10|             81|aancisgpjaueusynm...|\n",
      "|      18|         0|        0|2020-07-07|             62|gwkkxzjpdaaaskune...|\n",
      "|      19|         0|        0|2020-07-07|             13|jmltpvkcizhepIwwh...|\n",
      "|      20|         0|        0|2020-07-03|             69|sgicvswximmsqqtuj...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+\n",
      "|seller_id|seller_name|daily_target|\n",
      "+---------+-----------+------------+\n",
      "|        0|   seller_0|     2500000|\n",
      "|        1|   seller_1|     1375559|\n",
      "|        2|   seller_2|      205349|\n",
      "|        3|   seller_3|       71546|\n",
      "|        4|   seller_4|     1315668|\n",
      "|        5|   seller_5|      627802|\n",
      "|        6|   seller_6|     1997104|\n",
      "|        7|   seller_7|      593329|\n",
      "|        8|   seller_8|       24388|\n",
      "|        9|   seller_9|      348255|\n",
      "+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .appName(\"GenerateData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "products = spark.read.csv(\"data/products.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "products.show()\n",
    "products.write.parquet(\"data/products_parquet\", mode=\"overwrite\")\n",
    "\n",
    "sales = spark.read.csv(\"data/sales.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "sales.show()\n",
    "sales.repartition(200, col(\"product_id\")).write.parquet(\"data/sales_parquet\", mode=\"overwrite\")\n",
    "\n",
    "sellers = spark.read.csv(\"data/sellers.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "sellers.show()\n",
    "sellers.write.parquet(\"data/sellers_parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data into Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession as sqlSparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create session\n",
    "spark = sqlSparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read as Spark DF\n",
    "products_df = spark.read.parquet('./data/products_parquet')\n",
    "sales_df = spark.read.parquet('./data/sales_parquet')\n",
    "sellers_df = spark.read.parquet('./data/sellers_parquet')\n",
    "\n",
    "# Spark RDD\n",
    "products_rdd = products_df.rdd\n",
    "sales_rdd = sales_df.rdd\n",
    "sellers_rdd = sellers_df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Find out how many orders, how many products and how many sellers are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Products: 75,000,000\n",
      "Sales: 20,000,040\n",
      "Sellers: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark DF\n",
    "print(f\"\"\"\n",
    "Products: {products_df.count():,d}\n",
    "Sales: {sales_df.count():,d}\n",
    "Sellers: {sellers_df.count():,d}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================>                                    (30 + 8) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Products: 69,998,598\n",
      "Sales: 20,000,040\n",
      "Sellers: 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================>                                    (30 + 8) / 83]\r"
     ]
    }
   ],
   "source": [
    "# RDD\n",
    "products_count = products_rdd.count()\n",
    "sales_count = sales_rdd.count()\n",
    "sellers_count = sellers_rdd.count()\n",
    "\n",
    "print(f\"\"\"\n",
    "Products: {products_count:,d}\n",
    "Sales: {sales_count:,d}\n",
    "Sellers: {sellers_count:,d}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. How many products have been sold at least once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products sold at least once: 993,299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================>                                    (30 + 8) / 83]\r"
     ]
    }
   ],
   "source": [
    "# DF1\n",
    "products_sold = sales_df.drop_duplicates(['product_id']).count()\n",
    "print(f\"Products sold at least once: {products_sold:,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(product_id)|\n",
      "+-----------------+\n",
      "|           993299|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================>                                    (30 + 8) / 83]\r"
     ]
    }
   ],
   "source": [
    "# DF2\n",
    "sales_df.agg(countDistinct('product_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 149\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "[Stage 8:====================>                                    (30 + 8) / 83]\r"
     ]
    }
   ],
   "source": [
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rcadavid/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/rcadavid/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/g/My Drive/2-Ciencia_de_datos/Proyectos/Pyspark-review/Spark-exercises.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/g/My%20Drive/2-Ciencia_de_datos/Proyectos/Pyspark-review/Spark-exercises.ipynb#ch0000036vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# RDD\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/g/My%20Drive/2-Ciencia_de_datos/Proyectos/Pyspark-review/Spark-exercises.ipynb#ch0000036vscode-remote?line=1'>2</a>\u001b[0m sales_rdd\u001b[39m.\u001b[39;49mcountApproxDistinct()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:3200\u001b[0m, in \u001b[0;36mRDD.countApproxDistinct\u001b[0;34m(self, relativeSD)\u001b[0m\n\u001b[1;32m   3198\u001b[0m \u001b[39m# the hash space in Java is 2^32\u001b[39;00m\n\u001b[1;32m   3199\u001b[0m hashRDD \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: portable_hash(x) \u001b[39m&\u001b[39m \u001b[39m0xFFFFFFFF\u001b[39m)\n\u001b[0;32m-> 3200\u001b[0m \u001b[39mreturn\u001b[39;00m hashRDD\u001b[39m.\u001b[39;49m_to_java_object_rdd()\u001b[39m.\u001b[39;49mcountApproxDistinct(relativeSD)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================>                                    (30 + 8) / 83]\r"
     ]
    }
   ],
   "source": [
    "# RDD\n",
    "sales_rdd.map(lambda x: x[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Which is the product contained in more orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. How many distinct products have been sold in each day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|         0|        0|2020-07-03|             98|frlnwjcoaxsaubnat...|\n",
      "|       2|         0|        0|2020-07-07|             23|zsnrbwrlflvqqmbcz...|\n",
      "|       3|         0|        0|2020-07-02|             79|gmxnirkafafnohboh...|\n",
      "|       4|         0|        0|2020-07-07|              5|xrgknaskXkfcxcnzj...|\n",
      "|       5|         0|        0|2020-07-10|             79|tzkqoynsqnfomkpbt...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      date|count(1)|\n",
      "+----------+--------+\n",
      "|2020-07-03| 2001301|\n",
      "|2020-07-07| 2000202|\n",
      "|2020-07-01| 2001066|\n",
      "|2020-07-08| 2001015|\n",
      "|2020-07-04| 2001640|\n",
      "|2020-07-10| 1998316|\n",
      "|2020-07-09| 1999799|\n",
      "|2020-07-06| 2000492|\n",
      "|2020-07-02| 1999256|\n",
      "|2020-07-05| 1996953|\n",
      "+----------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================>                                    (30 + 8) / 83]\r"
     ]
    }
   ],
   "source": [
    "sales_df.groupby('date').agg({'*': 'count'}).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
